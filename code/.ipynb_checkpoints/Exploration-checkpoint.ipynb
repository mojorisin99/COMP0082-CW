{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d82f7bb1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5085a58b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T21:32:11.768885Z",
     "start_time": "2023-02-08T21:31:47.852396Z"
    }
   },
   "outputs": [],
   "source": [
    "import Bio\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e0c40b",
   "metadata": {},
   "source": [
    "## Create dictionary of sequences for each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb5f62bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T20:45:25.576377Z",
     "start_time": "2023-02-08T20:45:25.554710Z"
    }
   },
   "outputs": [],
   "source": [
    "blind_path = \"../data/blind.fasta.txt\"\n",
    "cyto_path = \"../data/cyto.fasta.txt\"\n",
    "mito_path = \"../data/mito.fasta.txt\"\n",
    "nucleus_path = \"../data/nucleus.fasta.txt\"\n",
    "other_path = \"../data/other.fasta.txt\"\n",
    "secreted_path = \"../data/secreted.fasta.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bc65482",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T20:45:26.518123Z",
     "start_time": "2023-02-08T20:45:26.501169Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_fasta(file):\n",
    "    \"\"\"\n",
    "    This function takes an unstructured fasta file and outputs a dictionary of the sequences\n",
    "    Input: - fasta file\n",
    "    Output: - dict with keys (sequence header) and values (sequence)\n",
    "    \"\"\"\n",
    "    sequences = {}\n",
    "    with open(file, 'r') as f:\n",
    "        header = \"\"\n",
    "        sequence = \"\"\n",
    "        for line in f:\n",
    "            #in a fasta file the first character is a > sign\n",
    "            if line[0] == \">\":\n",
    "                if header:\n",
    "                    sequences[header] = sequence\n",
    "                header = line[1:].strip()\n",
    "                sequence = \"\"\n",
    "            else:\n",
    "                sequence += line.strip()\n",
    "        sequences[header] = sequence\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01a3b0fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T20:45:27.248626Z",
     "start_time": "2023-02-08T20:45:27.154655Z"
    }
   },
   "outputs": [],
   "source": [
    "# This creates the dictionary of sequences for each location\n",
    "blind_sequences = read_fasta(blind_path)\n",
    "cyto_sequences = read_fasta(cyto_path)\n",
    "mito_sequences = read_fasta(mito_path)\n",
    "nucleus_sequences = read_fasta(nucleus_path)\n",
    "other_sequences = read_fasta(other_path)\n",
    "secreted_sequences = read_fasta(secreted_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdcbd80",
   "metadata": {},
   "source": [
    "## Exploring the suggested correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20efa5fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T20:45:27.993000Z",
     "start_time": "2023-02-08T20:45:27.985606Z"
    }
   },
   "outputs": [],
   "source": [
    "def seq_len(seq):\n",
    "    return len(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f746597",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T20:45:28.195317Z",
     "start_time": "2023-02-08T20:45:28.176319Z"
    }
   },
   "outputs": [],
   "source": [
    "def global_comp(seq):\n",
    "    analysed_seq = ProteinAnalysis(seq)\n",
    "    aa_comp = analysed_seq.count_amino_acids()\n",
    "    return aa_comp\n",
    "\n",
    "def aa_pct(seq):\n",
    "    analysed_seq = ProteinAnalysis(seq)\n",
    "    aa_pct = analysed_seq.get_amino_acids_percent()\n",
    "    return aa_pct\n",
    "\n",
    "def local_comp(seq, x,y):\n",
    "    seq = seq[x:y]\n",
    "    analysed_seq = ProteinAnalysis(seq)\n",
    "    aa_local_comp = analysed_seq.count_amino_acids()\n",
    "    return aa_local_comp\n",
    "\n",
    "def aa_local_pct(seq, x,y):\n",
    "    seq = seq[x:y]\n",
    "    analysed_seq = ProteinAnalysis(seq)\n",
    "    aa_local_pct = analysed_seq.get_amino_acids_percent()\n",
    "    return aa_local_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "727525db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T20:45:28.577519Z",
     "start_time": "2023-02-08T20:45:28.559504Z"
    }
   },
   "outputs": [],
   "source": [
    "def molecular_weight(seq):\n",
    "    analysed_seq = ProteinAnalysis(seq)\n",
    "    mw = analysed_seq.molecular_weight()\n",
    "    return mw\n",
    "\n",
    "def isoelectric_pt(seq):\n",
    "    analysed_seq = ProteinAnalysis(seq)\n",
    "    iso_pt = analysed_seq.isoelectric_point()\n",
    "    return iso_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6ae57e0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T11:59:13.505079Z",
     "start_time": "2023-02-08T11:59:13.494534Z"
    }
   },
   "outputs": [],
   "source": [
    "#for i in blind_sequences.values():\n",
    "    #print(aa_local_pct(i, 0,500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b6a9222",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T21:52:29.708905Z",
     "start_time": "2023-02-08T21:52:28.382342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/pierredemetz/.pyenv/versions/3.8.12/envs/bio/lib/python3.8/site-packages (1.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/pierredemetz/.pyenv/versions/3.8.12/envs/bio/lib/python3.8/site-packages (from torch) (4.4.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3f71a9",
   "metadata": {},
   "source": [
    "## Exploring other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1964cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddc5bf50",
   "metadata": {},
   "source": [
    "## Model building and training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56b403a",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9c279dc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T20:32:22.699914Z",
     "start_time": "2023-02-08T20:32:22.681533Z"
    }
   },
   "outputs": [],
   "source": [
    "class ProteinClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc(x)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1376fb0b",
   "metadata": {},
   "source": [
    "Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58dda96c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T21:53:06.187056Z",
     "start_time": "2023-02-08T21:53:05.647344Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAutoModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#import a tokenizer (parse text as numerical data and padding) along with a model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-cased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/bio/lib/python3.8/site-packages/transformers/utils/import_utils.py:1050\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1050\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/bio/lib/python3.8/site-packages/transformers/utils/import_utils.py:1038\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1036\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1038\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nAutoModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "#import a tokenizer (parse text as numerical data and padding) along with a model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d8f1de",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e53c20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
