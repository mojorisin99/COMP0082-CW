{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "37bb19f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "#model_def.py\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c18d4c",
   "metadata": {},
   "source": [
    "## Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a96c6d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "blind_path = \"../data/blind.fasta.txt\"\n",
    "cyto_path = \"../data/cyto.fasta.txt\"\n",
    "mito_path = \"../data/mito.fasta.txt\"\n",
    "nucleus_path = \"../data/nucleus.fasta.txt\"\n",
    "other_path = \"../data/other.fasta.txt\"\n",
    "secreted_path = \"../data/secreted.fasta.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11e7ad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fasta(file):\n",
    "    \"\"\"\n",
    "    This function takes an unstructured fasta file and outputs a dictionary of the sequences\n",
    "    Input: - fasta file\n",
    "    Output: - dict with keys (sequence header) and values (sequence)\n",
    "    \"\"\"\n",
    "    sequences = {}\n",
    "    with open(file, 'r') as f:\n",
    "        header = \"\"\n",
    "        sequence = \"\"\n",
    "        for line in f:\n",
    "            #in a fasta file the first character is a > sign\n",
    "            if line[0] == \">\":\n",
    "                if header:\n",
    "                    sequences[header] = sequence\n",
    "                header = line[1:].strip()\n",
    "                sequence = \"\"\n",
    "            else:\n",
    "                sequence += line.strip()\n",
    "        sequences[header] = sequence\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "602f963b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequences</th>\n",
       "      <th>cyto</th>\n",
       "      <th>mito</th>\n",
       "      <th>nucleus</th>\n",
       "      <th>other</th>\n",
       "      <th>secreted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MGQQVGRVGEAPGLQQPQPRGIRGSSAARPSGRRRDPAGRTADAGF...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MALEPIDYTTHSREIDAEYLKIVRGSDPDTTWLIISPNAKKEYEPE...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MNQIEPGVQYNYVYDEDEYMIQEEEWDRDLLLDPAWEKQQRKTFTA...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MSEEPTPVSGNDKQLLNKAWEITQKKTFTAWCNSHLRKLGSSIEQI...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MGDWMTVTDPGLSSESKTISQYTSETKMSPSSLYSQQVLCSSIPLS...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11219</th>\n",
       "      <td>MIPNITQLKTAALVMLFAGQALSGPVESRQASESIDAKFKAHGKKY...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11220</th>\n",
       "      <td>MLRKLVTGALAAALLLSGQSNAQNACQQTQQLSGGRTINNKNETGN...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11221</th>\n",
       "      <td>MIFHQFYSILILCLIFPNQVVQSDKERQDWIPSDYGGYMNPAGRSD...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11222</th>\n",
       "      <td>MKFQVVLSALLACSSAVVASPIENLFKYRAVKASHSKNINSTLPAW...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11223</th>\n",
       "      <td>MLTVALLALLCASASGNAIQARSSSYSGEYGGGGGKRFSHSGNQLD...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11224 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sequences  cyto  mito  nucleus  \\\n",
       "0      MGQQVGRVGEAPGLQQPQPRGIRGSSAARPSGRRRDPAGRTADAGF...     1     0        0   \n",
       "1      MALEPIDYTTHSREIDAEYLKIVRGSDPDTTWLIISPNAKKEYEPE...     1     0        0   \n",
       "2      MNQIEPGVQYNYVYDEDEYMIQEEEWDRDLLLDPAWEKQQRKTFTA...     1     0        0   \n",
       "3      MSEEPTPVSGNDKQLLNKAWEITQKKTFTAWCNSHLRKLGSSIEQI...     1     0        0   \n",
       "4      MGDWMTVTDPGLSSESKTISQYTSETKMSPSSLYSQQVLCSSIPLS...     1     0        0   \n",
       "...                                                  ...   ...   ...      ...   \n",
       "11219  MIPNITQLKTAALVMLFAGQALSGPVESRQASESIDAKFKAHGKKY...     0     0        0   \n",
       "11220  MLRKLVTGALAAALLLSGQSNAQNACQQTQQLSGGRTINNKNETGN...     0     0        0   \n",
       "11221  MIFHQFYSILILCLIFPNQVVQSDKERQDWIPSDYGGYMNPAGRSD...     0     0        0   \n",
       "11222  MKFQVVLSALLACSSAVVASPIENLFKYRAVKASHSKNINSTLPAW...     0     0        0   \n",
       "11223  MLTVALLALLCASASGNAIQARSSSYSGEYGGGGGKRFSHSGNQLD...     0     0        0   \n",
       "\n",
       "       other  secreted  \n",
       "0          0         0  \n",
       "1          0         0  \n",
       "2          0         0  \n",
       "3          0         0  \n",
       "4          0         0  \n",
       "...      ...       ...  \n",
       "11219      0         1  \n",
       "11220      0         1  \n",
       "11221      0         1  \n",
       "11222      0         1  \n",
       "11223      0         1  \n",
       "\n",
       "[11224 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This creates the dictionary of sequences for each location\n",
    "blind_sequences = read_fasta(blind_path)\n",
    "cyto_sequences = read_fasta(cyto_path)\n",
    "mito_sequences = read_fasta(mito_path)\n",
    "nucleus_sequences = read_fasta(nucleus_path)\n",
    "other_sequences = read_fasta(other_path)\n",
    "secreted_sequences = read_fasta(secreted_path)\n",
    "\n",
    "df_cyto = pd.DataFrame.from_dict(cyto_sequences, orient='index', columns=['Sequences'])\n",
    "df_cyto = df_cyto.reset_index().rename(columns={'index':'Label'})\n",
    "df_cyto['Label'] = 'cyto'\n",
    "\n",
    "df_mito = pd.DataFrame.from_dict(mito_sequences, orient='index', columns=['Sequences'])\n",
    "df_mito = df_mito.reset_index().rename(columns={'index':'Label'})\n",
    "df_mito['Label'] = 'mito'\n",
    "\n",
    "df_nucleus = pd.DataFrame.from_dict(nucleus_sequences, orient='index', columns=['Sequences'])\n",
    "df_nucleus = df_nucleus.reset_index().rename(columns={'index':'Label'})\n",
    "df_nucleus['Label'] = 'nucleus'\n",
    "\n",
    "df_other = pd.DataFrame.from_dict(other_sequences, orient='index', columns=['Sequences'])\n",
    "df_other = df_other.reset_index().rename(columns={'index':'Label'})\n",
    "df_other['Label'] = 'other'\n",
    "\n",
    "df_secreted = pd.DataFrame.from_dict(secreted_sequences, orient='index', columns=['Sequences'])\n",
    "df_secreted = df_secreted.reset_index().rename(columns={'index':'Label'})\n",
    "df_secreted['Label'] = 'secreted'\n",
    "\n",
    "df = pd.concat([df_cyto, df_mito, df_nucleus, df_other, df_secreted], axis=0).reset_index()\n",
    "# Display the DataFrame\n",
    "#df['encoded_cat'] = df['Label'].astype('category').cat.codes\n",
    "#df.drop(columns={'index', 'Label'}, inplace=True)\n",
    "\n",
    "#result = df.to_dict('records')\n",
    "\n",
    "one_hot = pd.get_dummies(df['Label'])\n",
    "df = pd.concat([df, one_hot], axis=1)\n",
    "df.drop(columns={'index', 'Label'}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "81527e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "leng = []\n",
    "for i in df['Sequences']:\n",
    "    leng.append(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d96ecfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = ['cyto', 'mito', 'nucleus','other', 'secreted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a734b0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, validation_set = False):  \n",
    "    \n",
    "    #train test split of dataset \n",
    "    train_size = 0.8\n",
    "    train_df=df.sample(frac=train_size,random_state=200)\n",
    "    test_df=df.drop(train_df.index).reset_index(drop=True)\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    \n",
    "    if validation_set:\n",
    "        #split the train further with a validation dataset\n",
    "        train_size = 0.8\n",
    "        train2_df = train_df.sample(frac=train_size, random_state=200)\n",
    "        val_df = train_df.drop(train2_df.index).reset_index(drop=True)\n",
    "        train2_df = train2_df.reset_index(drop=True)\n",
    "    \n",
    "        return train2_df, test_df, val_df\n",
    "   \n",
    "    return train_df, test_df\n",
    "\n",
    "train2_df, test_df, val_df = train_test_split(df, validation_set = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27ec1b6",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "68591714",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'Rostlab/prot_bert_bfd_localization'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f1ed57f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinSequenceDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequence)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        single_row = self.data.iloc[item]\n",
    "        sequence = single_row['Sequences']\n",
    "        target = single_row[target_list]\n",
    "        target[['cyto', 'mito', 'nucleus','other', 'secreted']] = target[['cyto', 'mito', 'nucleus','other', 'secreted']].astype(int)\n",
    "\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            sequence,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "          'protein_sequence': sequence,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a04e05c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ProteinSequenceDataset(train_df, tokenizer = tokenizer, max_len = 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e22ffd8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'protein_sequence': 'MLPGLAAAAAHRCSWSSLCRLRLRCRAAACNPSDRQEWQNLVTFGSFSNMVPCSHPYIGTLSQVKLYSTNVQKEGQGSQTLRVEKVPSFETAEGIGTELKAPLKQEPLQVRVKAVLKKREYGSKYTQNNFITGVRAINEFCLKSSDLEQLRKIRRRSPHEDTESFTVYLRSDVEAKSLEVWGSPEALAREKKLRKEAEIEYRERLFRNQKILREYRDFLGNTKPRSRTASVFFKGPGKVVMVAICINGLNCFFKFLAWIYTGSASMFSEAIHSLSDTCNQGLLALGISKSVQTPDPSHPYGFSNMRYISSLISGVGIFMMGAGLSWYHGVMGLLHPQPIESLLWAYCILAGSLVSEGATLLVAVNELRRNARAKGMSFYKYVMESRDPSTNVILLEDTAAVLGVIIAATCMGLTSITGNPLYDSLGSLGVGTLLGMVSAFLIYTNTEALLGRSIQPEQVQRLTELLENDPSVRAIHDVKATDLGLGKVRFKAEVDFDGRVVTRSYLEKQDFDQMLQEIQEVKTPEELETFMLKHGENIIDTLGAEVDRLEKELKKRNPEVRHVDLEIL',\n",
       " 'input_ids': tensor([2, 1, 3,  ..., 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       " 'targets': tensor([0, 0, 1, 0, 0])}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4ad763d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_df, test_df, tokenizer, batch_size=8, max_len=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = ProteinSequenceDataset(self.train_df, self.tokenizer, self.max_len)\n",
    "        self.test_dataset = ProteinSequenceDataset(self.test_df, self.tokenizer, self.max_len)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset,batch_size=self.batch_size,shuffle=True,num_workers=4)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_dataset,batch_size=1,num_workers=4)    \n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset,batch_size=1,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ea985118",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "data_module = ProteinDataModule(\n",
    "    train_df, \n",
    "    test_df, \n",
    "    tokenizer, \n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "data_module.setup() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b7eafd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'Rostlab/prot_bert_bfd_localization'\n",
    "class ProteinClassifier(pl.LightningModule):\n",
    "    def __init__(self, n_classes: int, steps_per_epoch=None, n_epochs=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.classifier = nn.Sequential(nn.Dropout(p=0.2),\n",
    "                                        nn.Linear(self.bert.config.hidden_size, n_classes),\n",
    "                                        nn.Tanh())\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.n_epochs = n_epochs\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        output = self.classifier(output.pooler_output)\n",
    "        output = torch.sigmoid(output)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels)\n",
    "        return loss, output\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"predictions\": outputs,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        labels = []\n",
    "        predictions = []\n",
    "        \n",
    "        for output in outputs:\n",
    "            for out_labels in output[\"labels\"].detach().cpu():\n",
    "                labels.append(out_labels)\n",
    "\n",
    "            for out_predictions in output[\"predictions\"].detach().cpu():\n",
    "                predictions.append(out_predictions)\n",
    "        print(\"#####\")\n",
    "        labels = torch.stack(labels).int()\n",
    "        predictions = torch.stack(predictions)\n",
    "\n",
    "        for i, name in enumerate(Classes):\n",
    "            roc_score = torchmetrics.AUROC(predictions[:, i], labels[:, i])\n",
    "            self.logger.experiment.add_scalar(f\"{name}_roc_auc/Train\", roc_score, self.current_epoch)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=2e-5)\n",
    "        warmup_steps = self.steps_per_epoch // 3\n",
    "        total_steps = self.steps_per_epoch * self.n_epochs - warmup_steps\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7dd58781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert_bfd_localization were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = ProteinClassifier(\n",
    "    n_classes=len(target_list), \n",
    "    steps_per_epoch=len(train_df)//BATCH_SIZE, \n",
    "    n_epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3cbf1738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "88288f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /Users/pierredemetz/UCL_work/COMP0082-CW/code/lightning_logs\n",
      "/Users/pierredemetz/.pyenv/versions/ucl/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | bert       | BertModel  | 419 M \n",
      "1 | classifier | Sequential | 5.1 K \n",
      "2 | criterion  | BCELoss    | 0     \n",
      "------------------------------------------\n",
      "419 M     Trainable params\n",
      "0         Non-trainable params\n",
      "419 M     Total params\n",
      "1,679.745 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98bf9088eb984815b869faeef3576bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'ProteinSequenceDataset' object has no attribute 'sequence'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/ucl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    606\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 608\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/ucl/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[38;5;241m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m~/.pyenv/versions/ucl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    643\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    646\u001b[0m     ckpt_path,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    647\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    648\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    649\u001b[0m )\n\u001b[0;32m--> 650\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/ucl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[0;32m-> 1112\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/.pyenv/versions/ucl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1191\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/ucl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1204\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_training_routine()\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;66;03m# enable train mode\u001b[39;00m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/ucl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1269\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;66;03m# reload dataloaders\u001b[39;00m\n\u001b[0;32m-> 1269\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reload_evaluation_dataloaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_sanity_val_batches \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_sanity_val_steps, val_batches) \u001b[38;5;28;01mfor\u001b[39;00m val_batches \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_val_batches\n\u001b[1;32m   1272\u001b[0m ]\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/ucl/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:234\u001b[0m, in \u001b[0;36mEvaluationLoop._reload_evaluation_dataloaders\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    232\u001b[0m     dataloaders \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtest_dataloaders\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mval_dataloaders \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39m_should_reload_val_dl:\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_val_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m     dataloaders \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mval_dataloaders\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataloaders \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/ucl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1649\u001b[0m, in \u001b[0;36mTrainer.reset_val_dataloader\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m   1643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m   1644\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msanity_checking \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mepoch_loop\u001b[38;5;241m.\u001b[39m_should_check_val_epoch())\n\u001b[1;32m   1645\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msanity_checking\n\u001b[1;32m   1646\u001b[0m ):\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_val_dl_reload_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_epoch\n\u001b[0;32m-> 1649\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_val_batches, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_dataloaders \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_connector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset_eval_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mRunningStage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVALIDATING\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpl_module\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/ucl/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:391\u001b[0m, in \u001b[0;36mDataConnector._reset_eval_dataloader\u001b[0;34m(self, mode, model)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dataloaders) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, dataloader \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloaders):\n\u001b[1;32m    390\u001b[0m         orig_num_batches \u001b[38;5;241m=\u001b[39m num_batches \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 391\u001b[0m             \u001b[38;5;28mlen\u001b[39m(dataloader) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mhas_len_all_ranks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    392\u001b[0m         )\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m orig_num_batches \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    395\u001b[0m             \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(orig_num_batches, \u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/ucl/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:106\u001b[0m, in \u001b[0;36mhas_len_all_ranks\u001b[0;34m(dataloader, strategy, model)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Checks if a given Dataloader has ``__len__`` method implemented i.e. if it is a finite dataloader or\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03minfinite dataloader.\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 106\u001b[0m     local_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore [arg-type] # we are checking with duck-typing\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     total_length \u001b[38;5;241m=\u001b[39m strategy\u001b[38;5;241m.\u001b[39mreduce(torch\u001b[38;5;241m.\u001b[39mtensor(local_length, device\u001b[38;5;241m=\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mroot_device), reduce_op\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m total_length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/ucl/lib/python3.8/site-packages/torch/utils/data/dataloader.py:480\u001b[0m, in \u001b[0;36mDataLoader.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m length\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_index_sampler\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/ucl/lib/python3.8/site-packages/torch/utils/data/sampler.py:272\u001b[0m, in \u001b[0;36mBatchSampler.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n",
      "File \u001b[0;32m~/.pyenv/versions/ucl/lib/python3.8/site-packages/torch/utils/data/sampler.py:79\u001b[0m, in \u001b[0;36mSequentialSampler.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_source\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[94], line 8\u001b[0m, in \u001b[0;36mProteinSequenceDataset.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequence\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ProteinSequenceDataset' object has no attribute 'sequence'"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d502a088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116a85a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6036ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfacdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'Rostlab/prot_bert_bfd_localization'\n",
    "class ProteinClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(ProteinClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.classifier = nn.Sequential(nn.Dropout(p=0.2),\n",
    "                                        nn.Linear(self.bert.config.hidden_size, n_classes),\n",
    "                                        nn.Tanh())\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bert(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask\n",
    "        )\n",
    "        return self.classifier(output.pooler_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "torch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
